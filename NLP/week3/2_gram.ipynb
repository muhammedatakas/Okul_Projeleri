from nltk import word_tokenize

sentence_tokens = []
for sentence in sentences:
  tokens = ["<bos>"] + [token.lower() for token in word_tokenize(sentence)] + ["<eos>"]
  sentence_tokens.append(tokens)

token_list = [x for X in sentence_tokens for x in X]
print(*sentence_tokens, sep="\n")