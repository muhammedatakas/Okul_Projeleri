{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am going to do my laundry\n",
      "I am going to meet my friends\n",
      "I am going to go to the store\n",
      "I am going to have lunch\n",
      "I am going to take a nap\n",
      "I am going to watch TV\n",
      "I am going to play video games\n",
      "I am going to go for a walk\n",
      "I am going to read a book\n",
      "I am going to call my parents\n",
      "I am going to write an email\n",
      "I am going to clean my room\n",
      "I am going to do my laundry\n",
      "I am going to cook dinner\n",
      "I am going to go to bed\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "continuations = [\n",
    "    \"meet my friends\",\n",
    "    \"go to the store\",\n",
    "    \"have lunch\",\n",
    "    \"take a nap\",\n",
    "    \"watch TV\",\n",
    "    \"play video games\",\n",
    "    \"go for a walk\",\n",
    "    \"read a book\",\n",
    "    \"call my parents\",\n",
    "    \"write an email\",\n",
    "    \"clean my room\",\n",
    "    \"do my laundry\",\n",
    "    \"cook dinner\",\n",
    "    \"go to bed\",\n",
    "]\n",
    "\n",
    "# Get a random continuation\n",
    "random_continuation = random.choice(continuations)\n",
    "\n",
    "# Generate a sentence\n",
    "sentence = \"I am going to \" + random_continuation\n",
    "\n",
    "# Print the sentence\n",
    "print(sentence)\n",
    "\n",
    "sentences = [\"I am going to \" + cont for cont in continuations]\n",
    "\n",
    "print(*sentences, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<bos>', 'i', 'am', 'going', 'to', 'meet', 'my', 'friends', '<eos>']\n",
      "['<bos>', 'i', 'am', 'going', 'to', 'go', 'to', 'the', 'store', '<eos>']\n",
      "['<bos>', 'i', 'am', 'going', 'to', 'have', 'lunch', '<eos>']\n",
      "['<bos>', 'i', 'am', 'going', 'to', 'take', 'a', 'nap', '<eos>']\n",
      "['<bos>', 'i', 'am', 'going', 'to', 'watch', 'tv', '<eos>']\n",
      "['<bos>', 'i', 'am', 'going', 'to', 'play', 'video', 'games', '<eos>']\n",
      "['<bos>', 'i', 'am', 'going', 'to', 'go', 'for', 'a', 'walk', '<eos>']\n",
      "['<bos>', 'i', 'am', 'going', 'to', 'read', 'a', 'book', '<eos>']\n",
      "['<bos>', 'i', 'am', 'going', 'to', 'call', 'my', 'parents', '<eos>']\n",
      "['<bos>', 'i', 'am', 'going', 'to', 'write', 'an', 'email', '<eos>']\n",
      "['<bos>', 'i', 'am', 'going', 'to', 'clean', 'my', 'room', '<eos>']\n",
      "['<bos>', 'i', 'am', 'going', 'to', 'do', 'my', 'laundry', '<eos>']\n",
      "['<bos>', 'i', 'am', 'going', 'to', 'cook', 'dinner', '<eos>']\n",
      "['<bos>', 'i', 'am', 'going', 'to', 'go', 'to', 'bed', '<eos>']\n",
      "<bos>\n",
      "i\n",
      "am\n",
      "going\n",
      "to\n",
      "meet\n",
      "my\n",
      "friends\n",
      "<eos>\n",
      "<bos>\n",
      "i\n",
      "am\n",
      "going\n",
      "to\n",
      "go\n",
      "to\n",
      "the\n",
      "store\n",
      "<eos>\n",
      "<bos>\n",
      "i\n",
      "am\n",
      "going\n",
      "to\n",
      "have\n",
      "lunch\n",
      "<eos>\n",
      "<bos>\n",
      "i\n",
      "am\n",
      "going\n",
      "to\n",
      "take\n",
      "a\n",
      "nap\n",
      "<eos>\n",
      "<bos>\n",
      "i\n",
      "am\n",
      "going\n",
      "to\n",
      "watch\n",
      "tv\n",
      "<eos>\n",
      "<bos>\n",
      "i\n",
      "am\n",
      "going\n",
      "to\n",
      "play\n",
      "video\n",
      "games\n",
      "<eos>\n",
      "<bos>\n",
      "i\n",
      "am\n",
      "going\n",
      "to\n",
      "go\n",
      "for\n",
      "a\n",
      "walk\n",
      "<eos>\n",
      "<bos>\n",
      "i\n",
      "am\n",
      "going\n",
      "to\n",
      "read\n",
      "a\n",
      "book\n",
      "<eos>\n",
      "<bos>\n",
      "i\n",
      "am\n",
      "going\n",
      "to\n",
      "call\n",
      "my\n",
      "parents\n",
      "<eos>\n",
      "<bos>\n",
      "i\n",
      "am\n",
      "going\n",
      "to\n",
      "write\n",
      "an\n",
      "email\n",
      "<eos>\n",
      "<bos>\n",
      "i\n",
      "am\n",
      "going\n",
      "to\n",
      "clean\n",
      "my\n",
      "room\n",
      "<eos>\n",
      "<bos>\n",
      "i\n",
      "am\n",
      "going\n",
      "to\n",
      "do\n",
      "my\n",
      "laundry\n",
      "<eos>\n",
      "<bos>\n",
      "i\n",
      "am\n",
      "going\n",
      "to\n",
      "cook\n",
      "dinner\n",
      "<eos>\n",
      "<bos>\n",
      "i\n",
      "am\n",
      "going\n",
      "to\n",
      "go\n",
      "to\n",
      "bed\n",
      "<eos>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\Muhammed\n",
      "[nltk_data]     Ata\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "sentence_tokens = []\n",
    "for sentence in sentences:\n",
    "  tokens = [\"<bos>\"] + [token.lower() for token in word_tokenize(sentence)] + [\"<eos>\"]\n",
    "  sentence_tokens.append(tokens)\n",
    "\n",
    "token_list = [x for X in sentence_tokens for x in X]\n",
    "print(*sentence_tokens, sep=\"\\n\")\n",
    "print(*token_list, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-gram freqs:\n",
      "(('<bos>', 'i', 'am'), 14)\n",
      "(('i', 'am', 'going'), 14)\n",
      "(('am', 'going', 'to'), 14)\n",
      "(('going', 'to', 'go'), 3)\n",
      "(('to', 'go', 'to'), 2)\n",
      "(('going', 'to', 'meet'), 1)\n",
      "(('to', 'meet', 'my'), 1)\n",
      "(('meet', 'my', 'friends'), 1)\n",
      "(('my', 'friends', '<eos>'), 1)\n",
      "(('go', 'to', 'the'), 1)\n",
      "(('to', 'the', 'store'), 1)\n",
      "(('the', 'store', '<eos>'), 1)\n",
      "(('going', 'to', 'have'), 1)\n",
      "(('to', 'have', 'lunch'), 1)\n",
      "(('have', 'lunch', '<eos>'), 1)\n",
      "(('going', 'to', 'take'), 1)\n",
      "(('to', 'take', 'a'), 1)\n",
      "(('take', 'a', 'nap'), 1)\n",
      "(('a', 'nap', '<eos>'), 1)\n",
      "(('going', 'to', 'watch'), 1)\n",
      "(('to', 'watch', 'tv'), 1)\n",
      "(('watch', 'tv', '<eos>'), 1)\n",
      "(('going', 'to', 'play'), 1)\n",
      "(('to', 'play', 'video'), 1)\n",
      "(('play', 'video', 'games'), 1)\n",
      "(('video', 'games', '<eos>'), 1)\n",
      "(('to', 'go', 'for'), 1)\n",
      "(('go', 'for', 'a'), 1)\n",
      "(('for', 'a', 'walk'), 1)\n",
      "(('a', 'walk', '<eos>'), 1)\n",
      "(('going', 'to', 'read'), 1)\n",
      "(('to', 'read', 'a'), 1)\n",
      "(('read', 'a', 'book'), 1)\n",
      "(('a', 'book', '<eos>'), 1)\n",
      "(('going', 'to', 'call'), 1)\n",
      "(('to', 'call', 'my'), 1)\n",
      "(('call', 'my', 'parents'), 1)\n",
      "(('my', 'parents', '<eos>'), 1)\n",
      "(('going', 'to', 'write'), 1)\n",
      "(('to', 'write', 'an'), 1)\n",
      "(('write', 'an', 'email'), 1)\n",
      "(('an', 'email', '<eos>'), 1)\n",
      "(('going', 'to', 'clean'), 1)\n",
      "(('to', 'clean', 'my'), 1)\n",
      "(('clean', 'my', 'room'), 1)\n",
      "(('my', 'room', '<eos>'), 1)\n",
      "(('going', 'to', 'do'), 1)\n",
      "(('to', 'do', 'my'), 1)\n",
      "(('do', 'my', 'laundry'), 1)\n",
      "(('my', 'laundry', '<eos>'), 1)\n",
      "(('going', 'to', 'cook'), 1)\n",
      "(('to', 'cook', 'dinner'), 1)\n",
      "(('cook', 'dinner', '<eos>'), 1)\n",
      "(('go', 'to', 'bed'), 1)\n",
      "(('to', 'bed', '<eos>'), 1)\n"
     ]
    }
   ],
   "source": [
    "# 3-gram frequencies\n",
    "threegrams = []\n",
    "for sentence in sentence_tokens:\n",
    "  for i in range(len(sentence)-2):\n",
    "    threegrams.append((sentence[i], sentence[i+1], sentence[i+2]))\n",
    "\n",
    "threegram_freqs = {x:0 for x in threegrams};\n",
    "for threegram in threegrams:\n",
    "  threegram_freqs[threegram] += 1;\n",
    "\n",
    "freqs = [(x,threegram_freqs[x]) for x in threegram_freqs]; # Convert the fdist to list\n",
    "freqs.sort(key=lambda t: t[1], reverse=1);\n",
    "\n",
    "print(\"3-gram freqs:\", *freqs, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-gram sentence generator function\n",
    "import random as rnd\n",
    "\n",
    "def generate_next_3grams(context):\n",
    "  curr_token_1, curr_token_2 = context[-2], context[-1]\n",
    "  possible_threegrams = [threegram for threegram in threegrams if threegram[0] == curr_token_1 and threegram[1] == curr_token_2]\n",
    "\n",
    "\n",
    "  if len(possible_threegrams) == 0:\n",
    "    return \"<eos>\"\n",
    "\n",
    "  sample_space = []\n",
    "  for threegram in possible_threegrams:\n",
    "    sample_space.append([threegram]*threegram_freqs[threegram])\n",
    "\n",
    "  sample_space = [x for X in sample_space for x in X]\n",
    "\n",
    "  next = rnd.choice(sample_space)\n",
    "\n",
    "  return next[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<bos>', 'i', 'am', 'going', 'to', 'go', 'to', 'bed', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "sentence = [\"<bos>\", \"i\"]\n",
    "\n",
    "while True:\n",
    "  next_token = generate_next_3grams(sentence).replace(' ', '')\n",
    "  sentence.append(next_token)\n",
    "  if next_token == \"<eos>\" or len(sentence) > 10:\n",
    "    break\n",
    "\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_school",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
